{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb2fc74",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'UnitCubeBound' from 'swyft.bounds' (/home/eliasd/lensing/swyft/swyft/bounds/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/scratch/ipykernel_4623/1389533034.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUniform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mswyft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounds\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnitCubeBound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mswyft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStateDictSaveable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mswyft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'UnitCubeBound' from 'swyft.bounds' (/home/eliasd/lensing/swyft/swyft/bounds/__init__.py)"
     ]
    }
   ],
   "source": [
    "from importlib import import_module\n",
    "from typing import Callable, Type, TypeVar, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from toolz import compose\n",
    "from toolz.dicttoolz import keyfilter\n",
    "from torch.distributions import Normal, Uniform\n",
    "\n",
    "from swyft.bounds import Bound, UnitCubeBound\n",
    "from swyft.saveable import StateDictSaveable\n",
    "from swyft.types import Array\n",
    "from swyft.utils import array_to_tensor, tensor_to_array\n",
    "\n",
    "PriorType = TypeVar(\"PriorType\", bound=\"Prior\")\n",
    "PriorTruncatorType = TypeVar(\"PriorTruncatorType\", bound=\"PriorTruncator\")\n",
    "\n",
    "\n",
    "class PriorTruncator(StateDictSaveable):\n",
    "    \"\"\"Samples from a truncated version of the prior and calculates the log_prob.\n",
    "\n",
    "    Args:\n",
    "        prior: Parameter prior\n",
    "        bound: Bound object\n",
    "\n",
    "    .. note::\n",
    "        The prior truncator is defined through a swyft.Bound object, which\n",
    "        sample from (subregions of) the hypercube, with swyft.Prior, which maps\n",
    "        the samples onto parameters of interest.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prior: \"Prior\", bound: Bound) -> None:\n",
    "        \"\"\"Instantiate prior truncator (combination of prior and bound).\n",
    "\n",
    "        Args:\n",
    "            prior: Prior object.\n",
    "            bound: Bound on hypercube.  Set 'None' for untruncated priors.\n",
    "        \"\"\"\n",
    "        self.prior = prior\n",
    "        if bound is None:\n",
    "            bound = UnitCubeBound(prior.n_parameters)\n",
    "        self.bound = bound\n",
    "\n",
    "    @property\n",
    "    def cdf(self) -> Callable:\n",
    "        return self.prior.cdf\n",
    "\n",
    "    @property\n",
    "    def icdf(self) -> Callable:\n",
    "        return self.prior.icdf\n",
    "\n",
    "    @property\n",
    "    def n_parameters(self) -> int:\n",
    "        return self.prior.n_parameters\n",
    "\n",
    "    def sample(self, n_samples: int) -> np.ndarray:\n",
    "        \"\"\"Sample from truncated prior.\n",
    "\n",
    "        Args:\n",
    "            n_samples: Number of samples to return\n",
    "\n",
    "        Returns:\n",
    "            Samples: (n_samples, n_parameters)\n",
    "        \"\"\"\n",
    "        u = self.bound.sample(n_samples)\n",
    "        return self.prior.icdf(u)\n",
    "\n",
    "    def log_prob(self, v: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Evaluate log probability.\n",
    "\n",
    "        Args:\n",
    "            v: (N, n_parameters) parameter points.\n",
    "\n",
    "        Returns:\n",
    "            log_prob: (N,)\n",
    "        \"\"\"\n",
    "        u = self.prior.cdf(v)\n",
    "        b = np.where(u.sum(axis=-1) == np.inf, 0.0, self.bound(u))\n",
    "        log_prob = np.where(\n",
    "            b == 0.0,\n",
    "            -np.inf,\n",
    "            self.prior.log_prob(v).sum(axis=-1) - np.log(self.bound.volume),\n",
    "        )\n",
    "        return log_prob\n",
    "\n",
    "    def state_dict(self) -> dict:\n",
    "        return dict(prior=self.prior.state_dict(), bound=self.bound.state_dict())\n",
    "\n",
    "    @classmethod\n",
    "    def from_state_dict(cls, state_dict: dict) -> PriorTruncatorType:\n",
    "        prior = Prior.from_state_dict(state_dict[\"prior\"])\n",
    "        bound = Bound.from_state_dict(state_dict[\"bound\"])\n",
    "        return cls(prior, bound)\n",
    "\n",
    "\n",
    "class InterpolatedTabulatedDistribution:\n",
    "    def __init__(self, icdf: Callable, n_parameters: int, n_grid_points: int) -> None:\n",
    "        r\"\"\"Create a distribution based off of a icdf. The distribution is defined by interpolating grid points.\n",
    "\n",
    "        Args:\n",
    "            icdf: inverse cumulative density function, aka ppf and uv\n",
    "            n_parameters: number of parameters, dimensionality of the prior\n",
    "            n_grid_points: number of grid points\n",
    "\n",
    "        .. warning::\n",
    "            Internally the mapping u -> v is tabulated on a linear grid on the\n",
    "            interval [0, 1], with `n` grid points. In extreme cases, this can\n",
    "            lead to approximation errors that can be mitigated by increasing\n",
    "            `n`.\n",
    "        \"\"\"\n",
    "        self.n_parameters = n_parameters\n",
    "        self._grid = np.linspace(0, 1.0, n_grid_points)\n",
    "        self._table = self._generate_table(icdf, self._grid, n_parameters)\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_table(\n",
    "        uv: Callable, grid: np.ndarray, n_parameters: int\n",
    "    ) -> np.ndarray:\n",
    "        table = []\n",
    "        for x in grid:\n",
    "            table.append(uv(np.ones(n_parameters) * x))\n",
    "        return np.array(table).T\n",
    "\n",
    "    def cdf(self, v: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Map onto hypercube: v -> u\n",
    "\n",
    "        Args:\n",
    "            v: (N, n_parameters) physical parameter array\n",
    "\n",
    "        Returns:\n",
    "            u: (N, n_parameters) hypercube parameter array\n",
    "        \"\"\"\n",
    "        u = np.empty_like(v)\n",
    "        for i in range(self.n_parameters):\n",
    "            u[:, i] = np.interp(\n",
    "                v[:, i], self._table[i], self._grid, left=np.inf, right=np.inf\n",
    "            )\n",
    "        return u\n",
    "\n",
    "    def icdf(self, u: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Map from hypercube: u -> v\n",
    "\n",
    "        Args:\n",
    "            u: (N, n_parameters) hypercube parameter array\n",
    "\n",
    "        Returns:\n",
    "            v: (N, n_parameters) physical parameter array\n",
    "        \"\"\"\n",
    "        v = np.empty_like(u)\n",
    "        for i in range(self.n_parameters):\n",
    "            v[:, i] = np.interp(\n",
    "                u[:, i], self._grid, self._table[i], left=np.inf, right=np.inf\n",
    "            )\n",
    "        return v\n",
    "\n",
    "    def log_prob(self, v: np.ndarray, du: float = 1e-6) -> np.ndarray:\n",
    "        \"\"\"Log probability.\n",
    "\n",
    "        Args:\n",
    "            v: (N, n_parameters) physical parameter array\n",
    "            du: Step-size of numerical derivatives\n",
    "\n",
    "        Returns:\n",
    "            log_prob: (N, n_parameters) factors of pdf\n",
    "        \"\"\"\n",
    "        dv = np.empty_like(v)\n",
    "        u = self.cdf(v)\n",
    "        for i in range(self.n_parameters):\n",
    "            dv[:, i] = np.interp(\n",
    "                u[:, i] + (du / 2), self._grid, self._table[i], left=None, right=None\n",
    "            )\n",
    "            dv[:, i] -= np.interp(\n",
    "                u[:, i] - (du / 2), self._grid, self._table[i], left=None, right=None\n",
    "            )\n",
    "        log_prob = np.where(u == np.inf, -np.inf, np.log(du) - np.log(dv + 1e-300))\n",
    "        return log_prob\n",
    "\n",
    "\n",
    "# TODO this could be improved with some thought\n",
    "# it merely wraps a torch distribution and keeps track of the arguments...\n",
    "class Prior(StateDictSaveable):\n",
    "    def __init__(\n",
    "        self, cdf: Callable, icdf: Callable, log_prob: Callable, n_parameters: int\n",
    "    ) -> None:\n",
    "        r\"\"\"Fully factorizable prior.\n",
    "\n",
    "        Args:\n",
    "            cdf: cumulative density function, aka vu\n",
    "            icdf: inverse cumulative density function, aka ppf and uv\n",
    "            log_prob: log density function\n",
    "            n_parameters: number of parameters / dimensionality of the prior\n",
    "\n",
    "        .. note::\n",
    "            The prior is defined through the mapping :math:`u\\to v`, from the\n",
    "            Uniform distribution, :math:`u\\sim \\text{Unif}(0, 1)` onto the\n",
    "            parameters of interest, :math:`v`.  This mapping corresponds to the\n",
    "            inverse cummulative distribution function, and is internally used\n",
    "            to perform inverse transform sampling.  Sampling happens in the\n",
    "            swyft.Bound object.\n",
    "        \"\"\"\n",
    "        self.cdf = cdf\n",
    "        self.icdf = icdf\n",
    "        self.log_prob = log_prob\n",
    "        self.n_parameters = n_parameters\n",
    "        self.method = \"__init__\"\n",
    "        self._state_dict = {\n",
    "            \"method\": self.method,\n",
    "            \"cdf\": self.cdf,\n",
    "            \"icdf\": self.icdf,\n",
    "            \"log_prob\": self.log_prob,\n",
    "            \"n_parameters\": self.n_parameters,\n",
    "        }\n",
    "        self.distribution = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_torch_distribution(\n",
    "        cls: Type[PriorType],\n",
    "        distribution: torch.distributions.Distribution,\n",
    "    ) -> PriorType:\n",
    "        r\"\"\"Create a prior from a batched pytorch distribution.\n",
    "\n",
    "        For example, ``distribution = torch.distributions.Uniform(-1 * torch.ones(5), 1 * torch.ones(5))``.\n",
    "\n",
    "        Args:\n",
    "            distribution: pytorch distribution\n",
    "\n",
    "        Returns:\n",
    "            Prior\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            len(distribution.batch_shape) == 1\n",
    "        ), f\"{distribution.batch_shape} must be one dimensional\"\n",
    "        assert (\n",
    "            len(distribution.event_shape) == 0\n",
    "        ), f\"{distribution} must be factorizable and report the log_prob of every dimension (i.e. all dims are in batch_shape)\"\n",
    "        prior = cls(\n",
    "            cdf=compose(tensor_to_array, distribution.cdf, array_to_tensor),\n",
    "            icdf=compose(tensor_to_array, distribution.icdf, array_to_tensor),\n",
    "            log_prob=compose(tensor_to_array, distribution.log_prob, array_to_tensor),\n",
    "            n_parameters=distribution.batch_shape.numel(),\n",
    "        )\n",
    "        prior.distribution = distribution\n",
    "        prior.method = \"from_torch_distribution\"\n",
    "        prior._state_dict = {\n",
    "            \"method\": prior.method,\n",
    "            \"name\": distribution.__class__.__name__,\n",
    "            \"module\": distribution.__module__,\n",
    "            \"kwargs\": keyfilter(\n",
    "                lambda x: x in distribution.__class__.arg_constraints,\n",
    "                distribution.__dict__,  # this depends on all relevant arguments being contained with prior.distribution.__class__.arg_constraints\n",
    "            ),\n",
    "        }\n",
    "        return prior\n",
    "\n",
    "    @classmethod\n",
    "    def from_uv(\n",
    "        cls, icdf: Callable, n_parameters: int, n_grid_points: int = 10_000\n",
    "    ) -> PriorType:\n",
    "        \"\"\"Create a prior which depends on ``InterpolatedTabulatedDistribution``, i.e. an interpolated representation of the icdf, cdf, and log_prob.\n",
    "\n",
    "        .. warning::\n",
    "            Internally the mapping u -> v is tabulated on a linear grid on the\n",
    "            interval [0, 1], with `n` grid points. In extreme cases, this can\n",
    "            lead to approximation errors that can be mitigated by increasing\n",
    "            `n` (in some cases).\n",
    "\n",
    "        Args:\n",
    "            icdf: map from hypercube: u -> v. inverse cumulative density function (icdf)\n",
    "            n_parameters: number of parameters / dimensionality of the prior\n",
    "            n_grid_points: number of grid points from which to interpolate the icdf, cdf, and log_prob\n",
    "\n",
    "        Returns:\n",
    "            Prior\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"This was too inaccurate.\")\n",
    "        distribution = InterpolatedTabulatedDistribution(\n",
    "            icdf, n_parameters, n_grid_points\n",
    "        )\n",
    "        prior = cls(\n",
    "            cdf=distribution.v,\n",
    "            icdf=distribution.u,\n",
    "            log_prob=distribution.log_prob,\n",
    "            n_parameters=n_parameters,\n",
    "        )\n",
    "        prior.distribution = distribution\n",
    "        prior.method = \"from_from_uv\"\n",
    "        prior.state_dict = None  # TODO, make like above.\n",
    "        return prior\n",
    "\n",
    "    def state_dict(self) -> dict:\n",
    "        return self._state_dict\n",
    "\n",
    "    @classmethod\n",
    "    def from_state_dict(cls, state_dict: dict) -> PriorType:\n",
    "        method = state_dict[\"method\"]\n",
    "\n",
    "        if method == \"__init__\":\n",
    "            kwargs = keyfilter(lambda x: x != \"method\", state_dict)\n",
    "            return cls(**kwargs)\n",
    "        elif method == \"from_torch_distribution\":\n",
    "            name = state_dict[\"name\"]\n",
    "            module = state_dict[\"module\"]\n",
    "            kwargs = state_dict[\"kwargs\"]\n",
    "            distribution = getattr(import_module(module), name)\n",
    "            distribution = distribution(**kwargs)\n",
    "            return getattr(cls, method)(distribution)\n",
    "        else:\n",
    "            NotImplementedError()\n",
    "\n",
    "\n",
    "def get_uniform_prior(low: Array, high: Array) -> Prior:\n",
    "    distribution = Uniform(array_to_tensor(low), array_to_tensor(high))\n",
    "    return Prior.from_torch_distribution(distribution)\n",
    "\n",
    "\n",
    "def get_diagonal_normal_prior(loc: Array, scale: Array) -> Prior:\n",
    "    distribution = Normal(array_to_tensor(loc), array_to_tensor(scale))\n",
    "    return Prior.from_torch_distribution(distribution)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
