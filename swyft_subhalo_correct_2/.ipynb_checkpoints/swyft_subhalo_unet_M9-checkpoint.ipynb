{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "636c170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch, pyro, numpy as np\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from clipppy import load_config, Clipppy\n",
    "from clipppy.patches import torch_numpy\n",
    "from ruamel.yaml import YAML\n",
    "\n",
    "import swyft\n",
    "import pyro.distributions as dist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "imkwargs = dict(extent=(-2.5, 2.5, -2.5, 2.5), origin='lower')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/eliasd/lensing/elias_utils')\n",
    "from plotting import *\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a58e86b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = '_m9_nsub5_nsim100'\n",
    "nsubstring = '-5M9'\n",
    "\n",
    "\n",
    "SYSTEM_NAME = \"ngc4414\"\n",
    "\n",
    "NSIM = 10000\n",
    "NSIM = 100\n",
    "SIM_PATH = f'/nfs/scratch/eliasd/store{RUN}.zarr' \n",
    "\n",
    "# UNet =  f'UNet{RUN}.pt'\n",
    "\n",
    "\n",
    "SIGMA = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr         = float(sys.argv[1])\n",
    "max_epochs = int(sys.argv[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beefe753",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec98d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(system_name: str, nsub: str = '') -> Clipppy:\n",
    "    \"\"\"\n",
    "    Get configuration\n",
    "    \"\"\"\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)  # HACK\n",
    "\n",
    "    SOURCE_DIR = '../../mock_data/sources'\n",
    "        \n",
    "    source_name = f'{system_name}.npy'\n",
    "    config = load_config(f'config-sub{nsub}.yaml', base_dir=SOURCE_DIR)\n",
    "\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)  # HACK\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_prior(config: Clipppy):\n",
    "    \"\"\"\n",
    "    Set up subhalo parameter priors using a config\n",
    "    \"\"\"\n",
    "    main = config.umodel.alphas[\"main\"]\n",
    "    prior_p_sub = main.sub.pos_sampler.base_dist\n",
    "    lows = np.array(\n",
    "        [\n",
    "            prior_p_sub.low[0].item(),\n",
    "            prior_p_sub.low[1].item(),\n",
    "        ]\n",
    "    )\n",
    "    highs = np.array(\n",
    "        [\n",
    "            prior_p_sub.high[0].item(),\n",
    "            prior_p_sub.high[1].item(),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    nsub = main.sub.nsub\n",
    "    lows = np.tile(lows, nsub)\n",
    "    highs = np.tile(highs, nsub)\n",
    "    \n",
    "    uv = lambda u: (highs - lows) * u + lows\n",
    "    \n",
    "    return swyft.Prior(uv, nsub*2), uv\n",
    "#     return swyft.Prior(uv, 2), uv\n",
    "\n",
    "\n",
    "def simul(v, config: Clipppy):\n",
    "    \"\"\"\n",
    "    Fix values for main lens and source parameters from config and put\n",
    "    in a subhalo with the specified position and mass.\n",
    "\n",
    "    Arguments\n",
    "    - v: array containing x_sub, y_sub.\n",
    "\n",
    "    Returns\n",
    "    - Numpy array.\n",
    "    \"\"\"\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)  # HACK\n",
    "    \n",
    "    from pyrofit.lensing.utils import get_meshgrid  # import here due to HACKs\n",
    "    nx = config.kwargs[\"defs\"][\"nx\"]\n",
    "    ny = config.kwargs[\"defs\"][\"ny\"]\n",
    "    res = config.kwargs[\"defs\"][\"res\"]\n",
    "    nsub = config.umodel.alphas[\"main\"].sub.nsub\n",
    "    X, Y = config.umodel.X.clone(), config.umodel.Y.clone()\n",
    "    # Upsample image\n",
    "    upsample = 10\n",
    "    config.umodel.coerce_XY(*get_meshgrid(res / upsample, nx * upsample, ny * upsample))\n",
    "        \n",
    "    if not torch.is_tensor(v):\n",
    "        v = torch.tensor(v)\n",
    "\n",
    "    xy_sub = v.view(-1,2).to(DEVICE)\n",
    "    d_p_sub = dist.Delta(xy_sub).to_event(1)\n",
    "    \n",
    "#     x_sub, y_sub = np.squeeze(v.T)\n",
    "#     d_p_sub = dist.Delta(torch.tensor([x_sub, y_sub])).to_event(1)\n",
    "\n",
    "\n",
    "    def _guide():\n",
    "        # Sample subhalo position\n",
    "        guide_sample = {\n",
    "            \"main/sub/p_sub\": pyro.sample(\"main/sub/p_sub\", d_p_sub),\n",
    "        }\n",
    "\n",
    "        return guide_sample\n",
    "    \n",
    "    result = {\n",
    "        \"image\": CONFIG.ppd(guide=_guide)[\"model_trace\"]\n",
    "        .nodes[\"mu\"][\"value\"]\n",
    "        .detach()\n",
    "        .numpy()\n",
    "    }\n",
    "    \n",
    "    # Restore coarse grid\n",
    "    config.umodel.coerce_XY(X, Y)\n",
    "    # Downsample image\n",
    "    averager = torch.nn.AvgPool2d((upsample, upsample))\n",
    "    result['image'] = (averager(torch.tensor(result['image']).unsqueeze(0).unsqueeze(0)).squeeze(0).squeeze(0))\n",
    "\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)  # HACK\n",
    "    return result\n",
    "\n",
    "def noise(obs, _=None, sigma_n=SIGMA):\n",
    "    image = obs[\"image\"]\n",
    "    eps = np.random.randn(*image.shape) * sigma_n\n",
    "    return {\"image\": image + eps}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f862fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[1, 2],\n",
    " [3, 4],\n",
    " [6, 7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570ff357",
   "metadata": {},
   "source": [
    "### Check utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeeac8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)  # HACK\n",
    "CONFIG = get_config(SYSTEM_NAME, nsubstring)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "217b6b54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)  # HACK\n",
    "ppd = CONFIG.ppd()['model_trace'].nodes\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c261f265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.4635, -0.0381],\n",
      "        [-2.2768,  0.3557],\n",
      "        [-0.3846, -0.9160],\n",
      "        [ 0.3412, -0.8713],\n",
      "        [-1.8368,  0.9490]], device='cuda:0')\n",
      "{'image': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# v = ppd['main/sub/p_sub']['value']\n",
    "# sim = simul(v, CONFIG)\n",
    "\n",
    "# print(v)\n",
    "# print(sim)\n",
    "\n",
    "# plt.scatter(*v.t(), c=\"r\")\n",
    "# plt.imshow(simul(v, CONFIG)['image'], **imkwargs)\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7769a9c",
   "metadata": {},
   "source": [
    "### Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7521dd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsub = 5\n"
     ]
    }
   ],
   "source": [
    "prior, uv = get_prior(CONFIG)\n",
    "nx = CONFIG.kwargs[\"defs\"][\"nx\"]\n",
    "ny = CONFIG.kwargs[\"defs\"][\"ny\"]\n",
    "nsub = CONFIG.umodel.alphas[\"main\"].sub.nsub\n",
    "print(f'nsub = {nsub}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f47d3930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x_1', 'y_1', 'x_2', 'y_2', 'x_3', 'y_3', 'x_4', 'y_4', 'x_5', 'y_5']\n",
      "Creating new store.\n",
      "Store: Adding 10006 new samples to simulator store.\n"
     ]
    }
   ],
   "source": [
    "pnames = [f'{z}_{i+1}' for i in range(nsub) for z in ['x', 'y']]\n",
    "print(pnames)\n",
    "simulator = swyft.Simulator(model = lambda v: simul(v, CONFIG), \n",
    "#                             pnames = [\"x_sub\", \"y_sub\"],\n",
    "                            pnames = pnames,\n",
    "                            sim_shapes={\"image\": (nx, ny)})\n",
    "\n",
    "store = swyft.DirectoryStore(path=SIM_PATH, simulator=simulator)\n",
    "# store = swyft.MemoryStore(simulator=simulator)\n",
    "\n",
    "store.add(NSIM, prior)\n",
    "store.simulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38128de",
   "metadata": {},
   "source": [
    "### Check store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "650c283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "coords_x = np.array([store[i][1][0::2] for i in range(N)])\n",
    "coords_y = np.array([store[i][1][1::2] for i in range(N)])\n",
    "imgs = np.array([store[i][0]['image'] for i in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6881a",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "732878f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L = 40\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "img_0 = store[idx][0]['image']\n",
    "L1, L2 = tensor(img_0.shape)\n",
    "assert L1 == L2\n",
    "L = L1.item()\n",
    "print(f'L = {L}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e022cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "dataset = swyft.Dataset(NSIM, prior, store)#, simhook = noise)\n",
    "marginals = [i for i in range(L**2)]\n",
    "# post = swyft.Posteriors(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1642d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord_uv(coords_u, lows, highs):\n",
    "#     highs_l = np.repeat(highs, coords_u)\n",
    "#     lows_l = np.repeat(lows, coords_u)\n",
    "    highs_l = np.full_like(coords_u, highs)\n",
    "    lows_l = np.full_like(coords_u, lows)\n",
    "    \n",
    "    v = lambda u: (highs_l - lows_l) * u + lows_l\n",
    "    coords_v = v(coords_u)\n",
    "    return coords_v\n",
    "\n",
    "def coord_to_map(XY_u):\n",
    "    \n",
    "    y0, y1, x0, x1 = -2.5, 2.5, -2.5, 2.5\n",
    "    lows, highs = -2.5, 2.5\n",
    "    res = 0.125\n",
    "    \n",
    "    XY = XY_u\n",
    "    \n",
    "    n_batch =  XY.shape[0]\n",
    "    n_coords = XY.shape[1]\n",
    "        \n",
    "    binary_map = torch.zeros((n_batch, L,L), device = DEVICE)\n",
    "    \n",
    "    x, y = XY[:,0::2], XY[:,1::2]\n",
    "    \n",
    "    x_i = torch.floor((x*L).flatten()).type(torch.long) \n",
    "    y_i = torch.floor((y*L).flatten()).type(torch.long) \n",
    "\n",
    "    if n_coords != 0:\n",
    "        i   = torch.floor(torch.arange(0, n_batch, 1/n_coords*2).to(DEVICE)).type(torch.long) \n",
    "    \n",
    "        xx = tuple(torch.stack((i, y_i, x_i)))\n",
    "        binary_map[xx] = 1\n",
    "\n",
    "    return binary_map\n",
    "    \n",
    "\n",
    "class DoubleConv(swyft.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False), # bias = False becaise BatchNorm2d is set\n",
    "            nn.BatchNorm2d(out_channels), # BatchNorm2d were not known when paper came out\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNET(swyft.Module):\n",
    "    def __init__(self, n_features, marginals):\n",
    "        super().__init__(n_features, marginals) \n",
    "#         super(UNET, self).__init__()\n",
    "        \n",
    "        self.marginals = marginals\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # keep size the same\n",
    "        \n",
    "        in_channels=1\n",
    "        out_channels=2\n",
    "        features=[64, 128, 256, 512]\n",
    "\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "        \n",
    "\n",
    "    def forward(self, sims, target):\n",
    "                \n",
    "        sims = sims.view(-1, L, L)\n",
    "        z = coord_to_map(target)\n",
    "    \n",
    "        ############# UNet Start ###\n",
    "        x = sims\n",
    "        n_batch = len(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1] # reverse list\n",
    "\n",
    "        # the upsampling\n",
    "        for idx in range(0, len(self.ups), 2): # step of 2 because we want up - double column - up - double column\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2] # //2 because we want still steps of one\n",
    "\n",
    "            # if statement because we can put in shapes that are not divisble by two around 19:00 of video\n",
    "            if x.shape != skip_connection.shape: \n",
    "                x = TF.resize(x, size=skip_connection.shape[2:]) # hopefully does not impact accuracy too much\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        ############# UNet End ###\n",
    "\n",
    "        \n",
    "                \n",
    "        # L[C]\n",
    "        x_new = x[:,0] * (1 - z) + x[:,1] * z\n",
    "        \n",
    "        \n",
    "        x = x_new\n",
    "        x = x.view(-1, self.n_features)\n",
    "        return x\n",
    "\n",
    "class CustomHead(swyft.Module):\n",
    "\n",
    "    def __init__(self, obs_shapes) -> None:\n",
    "        super().__init__(obs_shapes=obs_shapes)\n",
    "        self.n_features = torch.prod(tensor(obs_shapes['image']))\n",
    "\n",
    "    def forward(self, obs) -> torch.Tensor:\n",
    "        x = obs[\"image\"]\n",
    "        n_batch = len(x)\n",
    "        x = x.view(n_batch, self.n_features)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19c0785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses(post):\n",
    "        \n",
    "    keys = list(post._ratios.keys())\n",
    "    assert len(keys) == 1\n",
    "    losses = post._ratios[keys[0]]._train_diagnostics\n",
    "    assert len(losses) == 1\n",
    "    tl = losses[0]['train_loss']\n",
    "    vl = losses[0]['valid_loss']\n",
    "    epochs = np.arange(len(tl))\n",
    "    return epochs, tl, vl\n",
    "\n",
    "def plot_losses(post):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    \n",
    "    epochs, tl, vl = get_losses(post)\n",
    "        \n",
    "    ax.plot(epochs, tl, '--', label = f'training loss')\n",
    "    ax.plot(epochs, vl, '-', label = f'val loss')\n",
    "    \n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18e84718",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = np.power(10, -1.75)\n",
    "patience = 4\n",
    "\n",
    "save_id = f'lr{np.log10(lr)}_fac{np.log10(factor)}_pat{patience}'\n",
    "save_name = f'UNet_{save_id}.pt'\n",
    "save_path = os.path.join('posts_m9', save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08054c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training {save_name}!')\n",
    "    \n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "post = swyft.Posteriors(dataset)\n",
    "post.add(marginals, device = DEVICE, head = CustomHead, tail = UNET)\n",
    "post.train(marginals, max_epochs = max_epochs,\n",
    "           optimizer_args = dict(lr=lr),\n",
    "           scheduler_args = dict(factor = factor, patience = patience)\n",
    "          )\n",
    "\n",
    "post.save(save_path)\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
